{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMA_proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBSPCYttN7SW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcRT6RjLN7hz"
      },
      "source": [
        "\"alldat\" contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. The mouse had to determine which side has the highest contrast. For each dat = alldat[k], you have the following fields:\n",
        "\n",
        "* dat['spks']: neurons by trials by time bins. Time bin = 10ms.   \n",
        "* dat['brain_area']: brain area for each neuron recorded. \n",
        "* dat['contrast_right']: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
        "* dat['contrast_left']: contrast level for left stimulus. \n",
        "* dat['response']: which side the response was (-1, 0, 1). Choices for the right stimulus are -1.  \n",
        "* dat['response_times']: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and always does!). \n",
        "* dat['wheel']: exact position of the wheel that the mice uses to make a response, binned at 10ms. \n",
        "* dat['pupil']: pupil area  (noisy, because pupil is very small). \n",
        "* dat['lfp']: recording of the local field potential in each brain area from this experiment, binned at 10ms.\n",
        "* dat['brain_area_lfp']: brain area names for the LFP channels. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7UT7dyj_6R"
      },
      "source": [
        "`dat_LFP`, `dat_WAV`, `dat_ST` contain 39 sessions from 10 mice, data from Steinmetz et al, 2019, supplemental to the main data provided for NMA. Time bins for all measurements are 10ms, starting 500ms before stimulus onset (same as the main data). The followin fields are available across the three supplemental files. \n",
        "\n",
        "* `dat['lfp']`: recording of the local field potential in each brain area from this experiment, binned at `10ms`.\n",
        "* `dat['brain_area_lfp']`: brain area names for the LFP channels. \n",
        "* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\". \n",
        "* `dat['waveform_w']`: temporal components of spike waveforms. `w@u` reconstructs the time by channels action potential shape. \n",
        "* `dat['waveform_u]`: spatial components of spike waveforms.\n",
        "* `dat['ss']`: neurons by trials. Exact spikes times for each neuron and each trial, reference to the stimulus onset. A (neuron,trial) entry can be an empty list if that neuron did not fire at all on that trial. \n",
        "* `dat['%X%_passive']`: same as above for `X` = {`lfp`, `ss`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi0DDgf2c3fA"
      },
      "source": [
        "#@title Data retrieval : SPKS\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLWjKq8bLDqm",
        "cellView": "form"
      },
      "source": [
        "#@title Data retrieval  : LFP\n",
        "import os, requests\n",
        "\n",
        "lname = ['steinmetz_st.npz']\n",
        "lname.append('steinmetz_wav.npz')\n",
        "lname.append('steinmetz_lfp.npz')\n",
        "\n",
        "url = [\"https://osf.io/4bjns/download\"]\n",
        "url.append(\"https://osf.io/ugm9v/download\")\n",
        "url.append(\"https://osf.io/kx3v9/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(lname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(lname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJgjKPvmF6XX"
      },
      "source": [
        "#SPKS data loading\n",
        "import numpy as np\n",
        "import math \n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "  alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "#dat = alldat[3]\n",
        "#print(dat.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gbl2ZfuDcp"
      },
      "source": [
        "\n",
        "\n",
        "dat_LFP = np.load('steinmetz_lfp.npz', allow_pickle=True)['dat']\n",
        "#dat_WAV = np.load('steinmetz_wav.npz', allow_pickle=True)['dat']\n",
        "#dat_ST = np.load('steinmetz_st.npz', allow_pickle=True)['dat']\n",
        "\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "#\"\"\"LFP = dat_LFP[11]\n",
        "#print(LFP.keys())\n",
        "#WAV = dat_WAV[11]\n",
        "#print(WAV.keys())\n",
        "#SpT = dat_ST[11]\n",
        "#print(SpT.keys())\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5OZF1aheSRk"
      },
      "source": [
        "###dummy cel\n",
        "#np.concatenate(ba_dict[\"PL_R\"], axis =0)\n",
        "ba_dict['PL_R'] = [ba_dict['PL_R'][x] for x in range(len(ba_dict['PL_R'])) if np.shape(ba_dict['PL_R'][x]) == (1,30) ]\n",
        "ba_dict['PL_R'] = np.concatenate(ba_dict[\"PL_R\"], axis =0)\n",
        "avg = np.mean(ba_dict['PL_R']  , axis =0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZjJEQIdKgLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7363a543-f45e-4689-e53d-38000b2a416a"
      },
      "source": [
        "a = [0,1]\n",
        "power_dict['PL_L'][1][a]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.83254121, 17.30770661])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cvxyuQ1yuwe"
      },
      "source": [
        "sessions_ba = []\n",
        "for i in range(39):\n",
        "  LFP = dat_LFP[i]\n",
        "  if (req_ba[0] in LFP['brain_area_lfp']) and (req_ba[1] in LFP['brain_area_lfp']):\n",
        "    sessions_ba.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gfOJ4yljtfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01ab17e1-20ce-42b0-89db-19d7c4a08c3d"
      },
      "source": [
        "print(dat['response'][trial])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW-JRBLPreUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7825b3b4-7728-42b1-cdbf-0e4b5848b82f"
      },
      "source": [
        "#Function to extract lfp from brain areas into a dictionary\n",
        "# Get required brain area\n",
        "#d[\"string{0}\".format(x)] = \"Hello\"\n",
        "from scipy import signal\n",
        "req_ba = [\"PL\", \"MOs\"]\n",
        "\n",
        "dict = {'key1':'geeks', 'key2':'for'} \n",
        "ba_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "freq_dict =  {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "power_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "\n",
        "#Sessions with both the brain areas\n",
        "sessions_ba = []\n",
        "for i in range(39):\n",
        "  ba_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  freq_dict =  {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "  power_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "\n",
        "  LFP = dat_LFP[i]\n",
        "  if (req_ba[0] in LFP['brain_area_lfp']) and (req_ba[1] in LFP['brain_area_lfp']):\n",
        "    sessions_ba.append(i)\n",
        "\n",
        "#Select session of interest    \n",
        "sessions_ba = [4, 7, 11, 12, 24, 26, 35, 38]\n",
        "session = sessions_ba[0]\n",
        "\n",
        "LFP = dat_LFP[session]\n",
        "dat = alldat[session]\n",
        "PL_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"PL\"]\n",
        "MOs_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"MOs\"]\n",
        "  \n",
        "for trial in range(np.size(LFP['lfp'] , axis = 1)):\n",
        "    \n",
        "    if dat['response'][trial]== -1:\n",
        "       ba_dict['PL_R'].append(LFP['lfp'][PL_idx , trial , math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "       ba_dict['MOs_R'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "      \n",
        "    if dat['response'][trial] == 1:\n",
        "       ba_dict['PL_L'].append(LFP['lfp'][PL_idx , trial , math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "       ba_dict['MOs_L'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "    \n",
        "# convert list into arrays\n",
        "for i in ba_dict.keys():\n",
        "      \n",
        "     ba_dict[i] = [ba_dict[i][x] for x in range(len(ba_dict[i])) if np.shape(ba_dict[i][x]) == (1,30) ]\n",
        "     ba_dict[i] = np.concatenate(ba_dict[i], axis =0)\n",
        "#get separate trial data \n",
        "for i in ba_dict.keys():\n",
        "  for trial in range(len(ba_dict[i])):\n",
        "     \n",
        "     freqs, psd = signal.welch(ba_dict[i][trial] , fs = 100) \n",
        "     freq_dict[i].append(freqs)\n",
        "     power_dict[i].append(psd)\n",
        "\n",
        "\n",
        "#Average across trials\n",
        "PL_R = np.mean(ba_dict['PL_R']  , axis =0 )\n",
        "PL_L = np.mean(ba_dict['PL_L']  , axis =0 )\n",
        "MOs_R = np.mean(ba_dict['MOs_R']  , axis =0 )\n",
        "MOs_L = np.mean(ba_dict['MOs_L']  , axis =0 )\n",
        "#print(PL_L , PL_R , MOs_R , MOs_L)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 30, using nperseg = 30\n",
            "  .format(nperseg, input_length))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePuY5o_2qRO"
      },
      "source": [
        "Obtaining peak amplitude for different frequency bands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDgT8o33TU7a"
      },
      "source": [
        "#required frequency dict\n",
        "\n",
        "delta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "theta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "alpha_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "beta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "gamma_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "\n",
        "for i in freq_dict.keys():\n",
        "  for j in range(50): #(len(freq_dict[i])):\n",
        "    delta_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] <=4]\n",
        "    max_power = np.mean(power_dict[i][j][delta_idx])\n",
        "    delta_dict[i].append(max_power)\n",
        "\n",
        "    theta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 4) and (freq_dict[i][j][g] <= 8) ]\n",
        "    max_power = np.mean(power_dict[i][j][theta_idx])\n",
        "    theta_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "    alpha_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 8) and (freq_dict[i][j][g] <= 13)]\n",
        "    max_power = np.mean(power_dict[i][j][alpha_idx])\n",
        "    alpha_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "    beta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 13) and (freq_dict[i][j][g] <= 32)]\n",
        "    max_power = np.mean(power_dict[i][j][beta_idx])\n",
        "    beta_dict[i].append(max_power)\n",
        "\n",
        "    gamma_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] >32]\n",
        "    max_power = np.mean(power_dict[i][j][gamma_idx])\n",
        "    gamma_dict[i].append(max_power)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2r8c3FPfrC7"
      },
      "source": [
        "import pandas as pd\n",
        "# delta\n",
        "df_delta_Right = pd.DataFrame.from_dict((delta_dict['PL_R'] , delta_dict['MOs_R'])).transpose()\n",
        "df_delta_Right.columns = ['PL_R' , 'MOs_R'] \n",
        "df_delta_Left = pd.DataFrame.from_dict((delta_dict['PL_L'] , delta_dict['MOs_L'])).transpose()\n",
        "df_delta_Left.columns = ['PL_L' , 'MOs_L']\n",
        "corr_delta_R = df_delta_Right.corr()\n",
        "corr_delta_L = df_delta_Left.corr()\n",
        "\n",
        "\n",
        "#Theta\n",
        "df_theta_Right = pd.DataFrame.from_dict((theta_dict['PL_R'] , theta_dict['MOs_R'])).transpose()\n",
        "df_theta_Right.columns = ['PL_R' , 'MOs_R'] \n",
        "df_theta_Left = pd.DataFrame.from_dict((theta_dict['PL_L'] , theta_dict['MOs_L'])).transpose()\n",
        "df_theta_Left.columns = ['PL_L' , 'MOs_L']\n",
        "corr_theta_R = df_theta_Right.corr()\n",
        "corr_theta_L = df_theta_Left.corr()\n",
        "\n",
        "#Alpha\n",
        "df_alpha_Right = pd.DataFrame.from_dict((alpha_dict['PL_R'] , alpha_dict['MOs_R'])).transpose()\n",
        "df_alpha_Right.columns = ['PL_R' , 'MOs_R'] \n",
        "df_alpha_Left = pd.DataFrame.from_dict((alpha_dict['PL_L'] , alpha_dict['MOs_L'])).transpose()\n",
        "df_alpha_Left.columns = ['PL_L' , 'MOs_L']\n",
        "corr_alpha_R = df_alpha_Right.corr()\n",
        "corr_alpha_L = df_alpha_Left.corr()\n",
        "\n",
        "\n",
        "#Beta\n",
        "df_beta_Right = pd.DataFrame.from_dict((beta_dict['PL_R'] , beta_dict['MOs_R'])).transpose()\n",
        "df_beta_Right.columns = ['PL_R' , 'MOs_R'] \n",
        "df_beta_Left = pd.DataFrame.from_dict((beta_dict['PL_L'] , beta_dict['MOs_L'])).transpose()\n",
        "df_beta_Left.columns = ['PL_L' , 'MOs_L']\n",
        "corr_beta_R = df_beta_Right.corr()\n",
        "corr_beta_L = df_beta_Left.corr()\n",
        "\n",
        "\n",
        "#Gamma\n",
        "df_gamma_Right = pd.DataFrame.from_dict((gamma_dict['PL_R'] , gamma_dict['MOs_R'])).transpose()\n",
        "df_gamma_Right.columns = ['PL_R' , 'MOs_R'] \n",
        "df_gamma_Left = pd.DataFrame.from_dict((gamma_dict['PL_L'] , gamma_dict['MOs_L'])).transpose()\n",
        "df_gamma_Left.columns = ['PL_L' , 'MOs_L']\n",
        "corr_gamma_R = df_gamma_Right.corr()\n",
        "corr_gamma_L = df_gamma_Left.corr()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csgWn55Hn0U8"
      },
      "source": [
        "### **Active section!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2bdIuLwkQ5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd963aa0-e6aa-475c-cfa2-4b8189de2234"
      },
      "source": [
        "#####Current cell\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "#dictionaries for correlation\n",
        "right_dict = { 'delta': [] , 'alpha': [] , 'gamma' : [] , 'theta': [] , 'beta': []}\n",
        "left_dict = { 'delta': [] , 'alpha': [] , 'gamma' : [] , 'theta': [] , 'beta': []}\n",
        "\n",
        "\n",
        "#Select session of interest    \n",
        "sessions_ba = [4, 7, 11, 12, 24, 26, 35, 38]\n",
        "session = sessions_ba[0]\n",
        "for session in sessions_ba:\n",
        "  ba_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  freq_dict =  {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  power_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "  LFP = dat_LFP[session]\n",
        "  dat = alldat[session]\n",
        "  PL_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"PL\"]\n",
        "  MOs_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"MOs\"]\n",
        "  \n",
        "  for trial in range(np.size(LFP['lfp'] , axis = 1)):\n",
        "    \n",
        "    if dat['response'][trial]==-1:\n",
        "       ba_dict['PL_R'].append(LFP['lfp'][PL_idx , trial ,  math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "       ba_dict['MOs_R'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "      \n",
        "    if dat['response'][trial] == 1:\n",
        "       ba_dict['PL_L'].append(LFP['lfp'][PL_idx , trial , math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "       ba_dict['MOs_L'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "    \n",
        "# convert list into arrays\n",
        "  for t in ba_dict.keys():\n",
        "      \n",
        "     ba_dict[t] = [ba_dict[t][x] for x in range(len(ba_dict[t])) if np.shape(ba_dict[t][x]) == (1,30) ]\n",
        "     ba_dict[t] = np.concatenate(ba_dict[t], axis =0)\n",
        "#get separate trial data \n",
        "  for y in ba_dict.keys():\n",
        "    for trial in range(len(ba_dict[y])):\n",
        "     \n",
        "       freqs, psd = signal.welch(ba_dict[y][trial] , fs = 100) \n",
        "       freq_dict[y].append(freqs)\n",
        "       power_dict[y].append(psd)\n",
        "\n",
        "\n",
        "#required frequency dict\n",
        "\n",
        "  delta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  theta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  alpha_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  beta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  gamma_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "\n",
        "  for i in freq_dict.keys():\n",
        "    for j in range(50): #(len(freq_dict[i])):\n",
        "      delta_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] <=4]\n",
        "      max_power = np.mean(power_dict[i][j][delta_idx])\n",
        "      delta_dict[i].append(max_power)\n",
        "\n",
        "      theta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 4) and (freq_dict[i][j][g] <= 8) ]\n",
        "      max_power = np.mean(power_dict[i][j][theta_idx])\n",
        "      theta_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "      alpha_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 8) and (freq_dict[i][j][g] <= 13)]\n",
        "      max_power = np.mean(power_dict[i][j][alpha_idx])\n",
        "      alpha_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "      beta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 13) and (freq_dict[i][j][g] <= 32)]\n",
        "      max_power = np.mean(power_dict[i][j][beta_idx])\n",
        "      beta_dict[i].append(max_power)\n",
        "\n",
        "      gamma_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] >32]\n",
        "      max_power = np.mean(power_dict[i][j][gamma_idx])\n",
        "      gamma_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "\n",
        "# delta\n",
        "  \n",
        "  right_dict['delta'].append(np.corrcoef(delta_dict['PL_R'] , delta_dict['MOs_R'])[1][0])\n",
        "  left_dict['delta'].append(np.corrcoef(delta_dict['PL_L'] , delta_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Theta\n",
        "  \n",
        "  right_dict['theta'].append(np.corrcoef(theta_dict['PL_R'] , theta_dict['MOs_R'])[1][0])\n",
        "  left_dict['theta'].append(np.corrcoef(theta_dict['PL_L'] , theta_dict['MOs_L'])[1][0])\n",
        "\n",
        "#Alpha\n",
        "  \n",
        "  right_dict['alpha'].append(np.corrcoef(alpha_dict['PL_R'] , alpha_dict['MOs_R'])[1][0])\n",
        "  left_dict['alpha'].append(np.corrcoef(alpha_dict['PL_L'] , alpha_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Beta\n",
        "  \n",
        "  right_dict['beta'].append(np.corrcoef(beta_dict['PL_R'] , beta_dict['MOs_R'])[1][0])\n",
        "  left_dict['beta'].append(np.corrcoef(beta_dict['PL_L'] , beta_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Gamma\n",
        "  \n",
        "  right_dict['gamma'].append(np.corrcoef(gamma_dict['PL_R'] , gamma_dict['MOs_R'])[1][0])\n",
        "  left_dict['gamma'].append(np.corrcoef(gamma_dict['PL_L'] , gamma_dict['MOs_L'])[1][0])\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 30, using nperseg = 30\n",
            "  .format(nperseg, input_length))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oJndQeWpY_F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "31dcba55-789c-4b40-c4bb-40b5c80899df"
      },
      "source": [
        "Right = pd.DataFrame.from_dict(right_dict)\n",
        "Left = pd.DataFrame.from_dict(left_dict)\n",
        "print(Right)\n",
        "print(Left)\n",
        "print(right_dict)\n",
        "print(left_dict)\n",
        "#print(np.mean(right_dict.values))\n",
        "Left.to_csv(r'Left data_gr' , index = False , header = True, sep = '\\t' , )\n",
        "Right.to_csv(r'Right data_gr' , index = False , header = True, sep = '\\t' , )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      delta     alpha     gamma     theta      beta\n",
            "0  0.464359  0.766424  0.722970  0.584757  0.348375\n",
            "1  0.716892  0.630879  0.639067  0.485388  0.861553\n",
            "2  0.824516  0.935348  0.719441  0.791926  0.975412\n",
            "3  0.879125  0.982792  0.673972  0.963890  0.905622\n",
            "4  0.790464  0.803812  0.627032  0.834671  0.859020\n",
            "5  0.806112  0.907041  0.682273  0.893537  0.678772\n",
            "6  0.750138  0.781419  0.617568  0.770298  0.732765\n",
            "7  0.365446  0.754269  0.466976  0.603276  0.514194\n",
            "      delta     alpha     gamma     theta      beta\n",
            "0  0.648339  0.702383  0.603093  0.811366  0.426845\n",
            "1  0.734855  0.625821  0.731466  0.601775  0.631793\n",
            "2  0.840227  0.984467  0.585087  0.782604  0.978385\n",
            "3  0.809229  0.991519  0.589993  0.949767  0.988390\n",
            "4  0.833797  0.830033  0.815355  0.873920  0.860793\n",
            "5  0.748823  0.898609  0.704734  0.827903  0.789823\n",
            "6  0.743390  0.878983  0.499637  0.753196  0.684845\n",
            "7  0.479172  0.769403  0.390395  0.746885  0.605730\n",
            "{'delta': [0.4643591559974372, 0.7168921233875083, 0.8245164541459602, 0.8791254574968543, 0.7904638665450034, 0.8061118074931406, 0.7501383910708365, 0.3654464354558733], 'alpha': [0.7664237136413264, 0.6308790532608948, 0.9353477659612908, 0.9827919111203268, 0.8038117188545223, 0.907041402281034, 0.7814193782036555, 0.7542693999965976], 'gamma': [0.72296957151909, 0.6390673304645751, 0.7194407981214509, 0.6739716876952041, 0.6270321220359274, 0.6822728716646072, 0.6175681499412413, 0.46697564297034055], 'theta': [0.5847566888839211, 0.4853876010849244, 0.7919261603211257, 0.9638898759807181, 0.8346707945254264, 0.8935367851039031, 0.7702978021815748, 0.6032758884782553], 'beta': [0.34837495104278093, 0.8615528553894496, 0.9754122499888079, 0.9056224825558199, 0.8590195002854663, 0.6787716612448332, 0.7327648660916586, 0.5141937155258891]}\n",
            "{'delta': [0.6483387356355781, 0.7348551973879618, 0.8402271137821438, 0.8092285081944669, 0.8337966214549521, 0.748823240135313, 0.7433902180796846, 0.47917249958887387], 'alpha': [0.7023831030533852, 0.6258214691438136, 0.9844669831467422, 0.9915187515537788, 0.8300333676984305, 0.8986087092963576, 0.8789833180387531, 0.7694028161080154], 'gamma': [0.6030926264164248, 0.7314656261929499, 0.5850872756262225, 0.5899927800275806, 0.8153551470402798, 0.7047342734139646, 0.49963672544186255, 0.3903954798389727], 'theta': [0.8113656655867304, 0.6017749731246667, 0.7826041023973294, 0.9497672954529162, 0.8739204791113215, 0.8279033802436228, 0.7531956449709727, 0.7468854573349498], 'beta': [0.42684517732931104, 0.6317932209810239, 0.9783850661038739, 0.9883896158863025, 0.860792852001513, 0.7898231663098908, 0.6848450414612276, 0.6057300282215707]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5lQLhQwfQin"
      },
      "source": [
        "### MRN and MOs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDsVaVh5fNVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf6hSo8GfPIS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "65a8cdfc-cf8b-40dc-d2ff-682400ff62bc"
      },
      "source": [
        "#####Current cell\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#dictionaries for correlation\n",
        "right_dict = { 'delta': [] , 'alpha': [] , 'gamma' : [] , 'theta': [] , 'beta': []}\n",
        "left_dict = { 'delta': [] , 'alpha': [] , 'gamma' : [] , 'theta': [] , 'beta': []}\n",
        "\n",
        "\n",
        "#Select session of interest    \n",
        "sessions_ba = [12 , 13 , 21 , 24 , 25 , 31 , 35]\n",
        "session = sessions_ba[0]\n",
        "for session in sessions_ba:\n",
        "  ba_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  freq_dict =  {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  power_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "  LFP = dat_LFP[session]\n",
        "  dat = alldat[session]\n",
        "  PL_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"MRN\"]\n",
        "  MOs_idx = [g for g,v in enumerate(LFP['brain_area_lfp']) if LFP['brain_area_lfp'][g] == \"MOs\"]\n",
        "  \n",
        "  for trial in range(np.size(LFP['lfp'] , axis = 1)):\n",
        "    \n",
        "    if dat['response'][trial]==-1:\n",
        "       ba_dict['PL_R'].append(LFP['lfp'][PL_idx , trial , math.floor(dat['response_time'][trial]*100)-60 : math.floor(dat['response_time'][trial]*100)-30])\n",
        "       ba_dict['MOs_R'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-60 : math.floor(dat['response_time'][trial]*100)-30]) # math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "      \n",
        "    if dat['response'][trial] == 1:\n",
        "       ba_dict['PL_L'].append(LFP['lfp'][PL_idx , trial , math.floor(dat['response_time'][trial]*100)-60 : math.floor(dat['response_time'][trial]*100)-30])# math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "       ba_dict['MOs_L'].append(LFP['lfp'][MOs_idx ,  trial, math.floor(dat['response_time'][trial]*100)-60 : math.floor(dat['response_time'][trial]*100)-30]) # math.floor(dat['response_time'][trial]*100)-50 : math.floor(dat['response_time'][trial]*100)-20])\n",
        "    \n",
        "# convert list into arrays\n",
        "  for t in ba_dict.keys():\n",
        "      \n",
        "     ba_dict[t] = [ba_dict[t][x] for x in range(len(ba_dict[t])) if np.shape(ba_dict[t][x]) == (1,30) ]\n",
        "     ba_dict[t] = np.concatenate(ba_dict[t], axis =0)\n",
        "#get separate trial data \n",
        "  for y in ba_dict.keys():\n",
        "    for trial in range(len(ba_dict[y])):\n",
        "     \n",
        "       freqs, psd = signal.welch(ba_dict[y][trial] , fs = 100) \n",
        "       freq_dict[y].append(freqs)\n",
        "       power_dict[y].append(psd)\n",
        "\n",
        "\n",
        "#required frequency dict\n",
        "\n",
        "  delta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  theta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  alpha_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  beta_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "  gamma_dict = {'PL_R': [] , 'PL_L': [] , 'MOs_R': [] , 'MOs_L': []}\n",
        "\n",
        "\n",
        "  for i in freq_dict.keys():\n",
        "    for j in range(49): #(len(freq_dict[i])):\n",
        "      delta_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] <=4]\n",
        "      max_power = np.mean(power_dict[i][j][delta_idx])\n",
        "      delta_dict[i].append(max_power)\n",
        "\n",
        "      theta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 4) and (freq_dict[i][j][g] <= 8) ]\n",
        "      max_power = np.mean(power_dict[i][j][theta_idx])\n",
        "      theta_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "      alpha_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 8) and (freq_dict[i][j][g] <= 13)]\n",
        "      max_power = np.mean(power_dict[i][j][alpha_idx])\n",
        "      alpha_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "      beta_idx = [g for g,v in enumerate(freq_dict[i][j]) if (freq_dict[i][j][g] > 13) and (freq_dict[i][j][g] <= 32)]\n",
        "      max_power = np.mean(power_dict[i][j][beta_idx])\n",
        "      beta_dict[i].append(max_power)\n",
        "\n",
        "      gamma_idx = [g for g,v in enumerate(freq_dict[i][j]) if freq_dict[i][j][g] >32]\n",
        "      max_power = np.mean(power_dict[i][j][gamma_idx])\n",
        "      gamma_dict[i].append(max_power)\n",
        "\n",
        "\n",
        "\n",
        "# delta\n",
        "  \n",
        "  right_dict['delta'].append(np.corrcoef(delta_dict['PL_R'] , delta_dict['MOs_R'])[1][0])\n",
        "  left_dict['delta'].append(np.corrcoef(delta_dict['PL_L'] , delta_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Theta\n",
        "  \n",
        "  right_dict['theta'].append(np.corrcoef(theta_dict['PL_R'] , theta_dict['MOs_R'])[1][0])\n",
        "  left_dict['theta'].append(np.corrcoef(theta_dict['PL_L'] , theta_dict['MOs_L'])[1][0])\n",
        "\n",
        "#Alpha\n",
        "  \n",
        "  right_dict['alpha'].append(np.corrcoef(alpha_dict['PL_R'] , alpha_dict['MOs_R'])[1][0])\n",
        "  left_dict['alpha'].append(np.corrcoef(alpha_dict['PL_L'] , alpha_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Beta\n",
        "  \n",
        "  right_dict['beta'].append(np.corrcoef(beta_dict['PL_R'] , beta_dict['MOs_R'])[1][0])\n",
        "  left_dict['beta'].append(np.corrcoef(beta_dict['PL_L'] , beta_dict['MOs_L'])[1][0])\n",
        "\n",
        "\n",
        "#Gamma\n",
        "  \n",
        "  right_dict['gamma'].append(np.corrcoef(gamma_dict['PL_R'] , gamma_dict['MOs_R'])[1][0])\n",
        "  left_dict['gamma'].append(np.corrcoef(gamma_dict['PL_L'] , gamma_dict['MOs_L'])[1][0])\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 30, using nperseg = 30\n",
            "  .format(nperseg, input_length))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNI7KYbvhRsZ"
      },
      "source": [
        "Right = pd.DataFrame.from_dict(right_dict)\n",
        "Left = pd.DataFrame.from_dict(left_dict)\n",
        "print(Right)\n",
        "print(Left)\n",
        "print(right_dict)\n",
        "print(left_dict)\n",
        "#print(np.mean(right_dict.values))\n",
        "Left.to_csv(r'MRNLeft data' , index = False , header = True, sep = '\\t' , )\n",
        "Right.to_csv(r'MRNRight data' , index = False , header = True, sep = '\\t' , )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUxJNwLZSeKO"
      },
      "source": [
        "### Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-EICtZlSdFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8972671a-e114-4f8e-c198-f8d777a594a9"
      },
      "source": [
        "\n",
        "# Example of the Shapiro-Wilk Normality Test\n",
        "from scipy.stats import shapiro\n",
        "#data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
        "stat, p = shapiro(Left.values)\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably Gaussian')\n",
        "else:\n",
        "\tprint('Probably not Gaussian')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stat=0.970, p=0.367\n",
            "Probably Gaussian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JWcMNn_TfFU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "477e3368-f863-4c0a-e95a-da1b441e0351"
      },
      "source": [
        "\n",
        "# Example of the Analysis of Variance Test\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "stat, p = f_oneway(Right['delta'] , Right['gamma'] , Right['alpha'] , Right['beta'] , Right['theta'])\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably the same distribution')\n",
        "else:\n",
        "\tprint('Probably different distributions')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stat=1.309, p=0.286\n",
            "Probably the same distribution\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNd9mXvHW6hB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26c04955-3b77-44dd-abbf-fe13f19f1bd5"
      },
      "source": [
        "#t test\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "stat, p = ttest_ind(Right['theta'] , Left['theta'])\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably the same distribution')\n",
        "else:\n",
        "\tprint('Probably different distributions')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stat=-0.760, p=0.460\n",
            "Probably the same distribution\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}